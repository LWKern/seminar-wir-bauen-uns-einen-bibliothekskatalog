# 10.1 Referat zu Relevanzrankings

## Kernpunkte

Aus dem Referat von Dana Curic und Christian Dublasky am 19.12.2016 (vgl. [Präsentationsfolien](https://felixlohmeier.gitbooks.io/seminar-wir-bauen-uns-einen-bibliothekskatalog/content/slides/10_1_referat_zu_relevanzrankings.pdf)) und der anschließenden Diskussion:

* Relevanz ist abhängig vom Nutzungskontext, d.h. von der Person, die sucht und von ihrem Erkenntnisinteresse zum jeweiligen Zeitpunkt. Es ist daher nahezu unmöglich eine objektive Relevanz zu definieren.
* Discovery-Systeme versuchen eine subjektiv als "gut" empfundene Relevanzsortierung durch eine unterschiedliche Gewichtung der verschiedenen Metadatenfelder (Titel, UrheberIn, Beschreibungstext) herzustellen.
* Wenn die Daten uneinheitlich sind (z.B. wenn zu einem Objekt viele und zu einem anderen Objekt sehr wenige beschreibende Daten vorliegen), dann führt dies oft zu unerwarteten Rankings, weil der Suchindex in der Standardkonfiguration das Verhältnis der Suchtreffer in einem Dokument zur Gesamtlänge des Dokuments in die Berechnung der Relevanzsortierung einfließen lässt.
* Weil die Definition eines Algorithmus auf Basis von objektiven Kriterien so schwer fällt, wird in der Praxis die Gewichtung der Felder oft experimentell auf Basis von häufig durchgeführten Suchen austariert. Nutzerstudien sind beim Relevanzranking also besonders wichtig.

## Literatur

* Präsentation von Elmar Haake: Relevanzranking als Erfolgsfaktor für Discoverysysteme. http://docplayer.org/3530893-Relevanzranking-als-erfolgsfaktor-fuer-discoverysysteme-elmar-haake-staats-und-universitaetsbibliothek-bremen.html
* Hajo Seng: Relevance-Ranking auf dem VuFind-Anwendertreffen (siehe die dort verlinkten Papiere und Vortragsfolien). http://beluga-blog.sub.uni-hamburg.de/blog/2015/10/02/relevance-ranking-auf-dem-vufind-anwendertreffen-2015/
